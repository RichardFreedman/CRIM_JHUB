{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f7b6937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_csv folder already exists.\n",
      "Music_Files folder already exists.\n"
     ]
    }
   ],
   "source": [
    "import intervals\n",
    "from intervals import * \n",
    "from intervals import main_objs\n",
    "import intervals.visualizations as viz\n",
    "import pandas as pd\n",
    "import re\n",
    "import altair as alt \n",
    "from ipywidgets import interact\n",
    "from pandas.io.json import json_normalize\n",
    "from pyvis.network import Network\n",
    "import glob as glob\n",
    "import os\n",
    "from IPython.display import SVG\n",
    "#from math import nan, isnan  # I think I don't need it anymore\n",
    "from itertools import combinations # added this one too.\n",
    "\n",
    "MYDIR = (\"saved_csv\")\n",
    "CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not CHECK_FOLDER:\n",
    "    os.makedirs(MYDIR)\n",
    "    print(\"created folder : \", MYDIR)\n",
    "\n",
    "else:\n",
    "    print(MYDIR, \"folder already exists.\")\n",
    "    \n",
    "MUSDIR = (\"Music_Files\")\n",
    "CHECK_FOLDER = os.path.isdir(MUSDIR)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not CHECK_FOLDER:\n",
    "    os.makedirs(MUSDIR)\n",
    "    print(\"created folder : \", MUSDIR)\n",
    "\n",
    "else:\n",
    "    print(MUSDIR, \"folder already exists.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7aef443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance as td\n",
    "\n",
    "ALGORITHMS = [\n",
    "    \"Levenshtein\",\n",
    "    \"Hamming\",\n",
    "]\n",
    "\n",
    "CALCULATIONS = [\"Distance\", \"Similarity\"]\n",
    "\n",
    "\n",
    "algorithm_to_apply = {\n",
    "        (\"Levenshtein\", \"Distance\", True): td.levenshtein.normalized_distance,\n",
    "        (\"Levenshtein\", \"Similarity\", True): td.levenshtein.normalized_similarity,\n",
    "        (\"Levenshtein\", \"Distance\", False): td.levenshtein.distance,\n",
    "        (\"Levenshtein\", \"Similarity\", False): td.levenshtein.similarity,\n",
    "        \n",
    "        (\"Hamming\", \"Distance\", True): td.hamming.normalized_distance,\n",
    "        (\"Hamming\", \"Similarity\", True): td.hamming.normalized_similarity,\n",
    "        (\"Hamming\", \"Distance\", False): td.hamming.distance,\n",
    "        (\"Hamming\", \"Similarity\", False): td.hamming.similarity,\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9fb3fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously imported piece detected.\n",
      "Roland de Lassus 's Missa Doulce memoire: Kyrie has 4 voices.\n"
     ]
    }
   ],
   "source": [
    "#import one of your pieces for voice number detector\n",
    "\n",
    "#4vv\n",
    "piece = importScore('https://crimproject.org/mei/CRIM_Mass_0033_1.mei')\n",
    "\n",
    "#5vv\n",
    "#piece = importScore('https://crimproject.org/mei/CRIM_Mass_0020_1.mei')\n",
    "#6vv\n",
    "#piece = importScore('https://crimproject.org/mei/CRIM_Mass_0015_1.mei')\n",
    "\n",
    "voice_number = len(piece.notes().columns)\n",
    "print(piece.metadata['composer'],\"'s\", piece.metadata['title'], 'has', voice_number, 'voices.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcfa8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously imported piece detected.\n",
      "Previously imported piece detected.\n",
      "Previously imported piece detected.\n",
      "Previously imported piece detected.\n",
      "Downloading remote score...\n",
      "Error downloading https://crimproject.org/mei/CRIM_Mass_0033_4.mei, please check your url and try again. Continuing to next file.\n",
      "Previously imported piece detected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#5vv\\ncorpus = CorpusBase(['https://crimproject.org/mei/CRIM_Mass_0020_1.mei',\\n                     'https://crimproject.org/mei/CRIM_Mass_0020_2.mei',\\n                     'https://crimproject.org/mei/CRIM_Mass_0020_3.mei',\\n                     'https://crimproject.org/mei/CRIM_Mass_0020_4.mei',\\n                     'https://crimproject.org/mei/CRIM_Mass_0020_5.mei'])   \\n                     \\n#6vv\\ncorpus = CorpusBase(['https://crimproject.org/mei/CRIM_Mass_0015_1.mei',\\n                     'https://crimproject.org/mei/CRIM_Mass_0015_2.mei',\\n                     'https://crimproject.org/mei/CRIM_Mass_0015_3.mei',\\n                     'https://crimproject.org/mei/CRIM_Mass_0015_4.mei',\\n                     'https://crimproject.org/mei/CRIM_Mass_0015_5.mei'])                  \\n              \\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#piece = importScore('Music_Files/CRIM_Model_0033.mei')\n",
    "#piece = importScore('https://crimproject.org/mei/CRIM_Model_0003.mei')\n",
    "#corpus = CorpusBase(['Music_Files/CRIM_Model_0033.mei'])\n",
    "#corpus = CorpusBase(['https://crimproject.org/mei/CRIM_Model_0001.mei'])\n",
    "\n",
    "#4vv\n",
    "corpus = CorpusBase(['https://crimproject.org/mei/CRIM_Model_0033.mei', \n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0033_1.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0033_2.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0033_3.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0033_4.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0033_5.mei'])  \n",
    "\n",
    "\"\"\"\n",
    "#5vv\n",
    "corpus = CorpusBase(['https://crimproject.org/mei/CRIM_Mass_0020_1.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0020_2.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0020_3.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0020_4.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0020_5.mei'])   \n",
    "                     \n",
    "#6vv\n",
    "corpus = CorpusBase(['https://crimproject.org/mei/CRIM_Mass_0015_1.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0015_2.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0015_3.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0015_4.mei',\n",
    "                     'https://crimproject.org/mei/CRIM_Mass_0015_5.mei'])                  \n",
    "              \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aead474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "#pd.set_option('display.max_rows', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9362229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>4_3</th>\n",
       "      <th>4_2</th>\n",
       "      <th>4_1</th>\n",
       "      <th>3_2</th>\n",
       "      <th>3_1</th>\n",
       "      <th>2_1</th>\n",
       "      <th>Composer</th>\n",
       "      <th>Title</th>\n",
       "      <th>figures</th>\n",
       "      <th>figures_combined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Measure</th>\n",
       "      <th>Beat</th>\n",
       "      <th>Offset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>583</td>\n",
       "      <td>[583, 358, 583, 358, 583, 050, 058, 803, 804, 633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>1.0</th>\n",
       "      <th>8.0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>358</td>\n",
       "      <td>[358, 583, 358, 583, 050, 058, 803, 804, 633, 638]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <th>10.0</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>583</td>\n",
       "      <td>[583, 358, 583, 050, 058, 803, 804, 633, 638, 527]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <th>12.0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>358</td>\n",
       "      <td>[358, 583, 050, 058, 803, 804, 633, 638, 527, 335]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>1.0</th>\n",
       "      <th>16.0</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>583</td>\n",
       "      <td>[583, 050, 058, 803, 804, 633, 638, 527, 335, 453]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <th>20.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>050</td>\n",
       "      <td>[050, 058, 803, 804, 633, 638, 527, 335, 453, 363]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <th>22.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>058</td>\n",
       "      <td>[058, 803, 804, 633, 638, 527, 335, 453, 363, 584]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>1.0</th>\n",
       "      <th>24.0</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>803</td>\n",
       "      <td>[803, 804, 633, 638, 527, 335, 453, 363, 584, 583]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5</th>\n",
       "      <th>27.0</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>804</td>\n",
       "      <td>[804, 633, 638, 527, 335, 453, 363, 584, 583, 838]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <th>28.0</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>Pierre Sandrin</td>\n",
       "      <td>Doulce memoire</td>\n",
       "      <td>633</td>\n",
       "      <td>[633, 638, 527, 335, 453, 363, 584, 583, 838, 583]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    4_3 4_2 4_1 3_2 3_1 2_1        Composer           Title  \\\n",
       "Measure Beat Offset                                                           \n",
       "1       1.0  0.0      5   8   3   4   6   3  Pierre Sandrin  Doulce memoire   \n",
       "2       1.0  8.0      3   5   8   3   6   4  Pierre Sandrin  Doulce memoire   \n",
       "        2.0  10.0     5   8   3   4   6   3  Pierre Sandrin  Doulce memoire   \n",
       "        3.0  12.0     3   5   8   3   6   4  Pierre Sandrin  Doulce memoire   \n",
       "3       1.0  16.0     5   8   3   4   6   3  Pierre Sandrin  Doulce memoire   \n",
       "        3.0  20.0     0   0   0   5   0   0  Pierre Sandrin  Doulce memoire   \n",
       "        4.0  22.0     0   0   0   5   8   4  Pierre Sandrin  Doulce memoire   \n",
       "4       1.0  24.0     8   0   3   0   3   0  Pierre Sandrin  Doulce memoire   \n",
       "        2.5  27.0     8   0   4   0   4   0  Pierre Sandrin  Doulce memoire   \n",
       "        3.0  28.0     6   3   3   4   5   8  Pierre Sandrin  Doulce memoire   \n",
       "\n",
       "                    figures  \\\n",
       "Measure Beat Offset           \n",
       "1       1.0  0.0        583   \n",
       "2       1.0  8.0        358   \n",
       "        2.0  10.0       583   \n",
       "        3.0  12.0       358   \n",
       "3       1.0  16.0       583   \n",
       "        3.0  20.0       050   \n",
       "        4.0  22.0       058   \n",
       "4       1.0  24.0       803   \n",
       "        2.5  27.0       804   \n",
       "        3.0  28.0       633   \n",
       "\n",
       "                                                       figures_combined  \n",
       "Measure Beat Offset                                                      \n",
       "1       1.0  0.0     [583, 358, 583, 358, 583, 050, 058, 803, 804, 633]  \n",
       "2       1.0  8.0     [358, 583, 358, 583, 050, 058, 803, 804, 633, 638]  \n",
       "        2.0  10.0    [583, 358, 583, 050, 058, 803, 804, 633, 638, 527]  \n",
       "        3.0  12.0    [358, 583, 050, 058, 803, 804, 633, 638, 527, 335]  \n",
       "3       1.0  16.0    [583, 050, 058, 803, 804, 633, 638, 527, 335, 453]  \n",
       "        3.0  20.0    [050, 058, 803, 804, 633, 638, 527, 335, 453, 363]  \n",
       "        4.0  22.0    [058, 803, 804, 633, 638, 527, 335, 453, 363, 584]  \n",
       "4       1.0  24.0    [803, 804, 633, 638, 527, 335, 453, 363, 584, 583]  \n",
       "        2.5  27.0    [804, 633, 638, 527, 335, 453, 363, 584, 583, 838]  \n",
       "        3.0  28.0    [633, 638, 527, 335, 453, 363, 584, 583, 838, 583]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_length = 10\n",
    "\n",
    "func1 = ImportedPiece.harmonic\n",
    "list_of_dfs = corpus.batch(func=func1, kwargs={'kind': 'd', 'compound':False, 'directed':False}, metadata=False) # I added 'directed':False to avoid negatives and be able to have integers but this can create false exact matches \n",
    "func2 = ImportedPiece.detailIndex\n",
    "list_of_detail_index = corpus.batch(func=func2, kwargs={'offset':True,'df': list_of_dfs})\n",
    "\n",
    "cleaned_list = []\n",
    "\n",
    "for harm in list_of_detail_index: #for each df sitting in the list applying these functions (for each df in list, can apply df functions, but not on list as a whole)\n",
    "    harm = harm.fillna(method=\"pad\")\n",
    "    harm = harm.replace('Rest', '0')\n",
    "    harm['figures'] = harm[harm.columns[:voice_number-1]].apply(lambda x: ''.join(x.astype(str)), axis=1) #a default 'figures' value if lowest voice is sounding\n",
    "        #harm['figures'] = harm[harm.columns[:3]].apply(lambda x: ''.join(x.astype(str)), axis=1)\n",
    "        #harm['figures'] = harm[harm.columns[0]].astype(str) + harm[harm.columns[1]].astype(str) + harm[harm.columns[2]].astype(str)\n",
    "    \n",
    "    if voice_number == 4:\n",
    "        #v4_rests = (harm.iloc[:, 0:3] == '0') THIS DOESNT WORK\n",
    "        v4_rests = (harm.iloc[:,0] == '0') & (harm.iloc[:,1] == '0') & (harm.iloc[:,2] == '0') # .iloc[:,0] == 'Rest'  means taking all rows (:) in column zero (,0) then filtering (==Rest) for where the row is Rest\n",
    "        harm.loc[v4_rests, 'figures'] = harm.loc[v4_rests, harm.columns[0]].astype(str) + harm.loc[v4_rests, harm.columns[3]].astype(str) + harm.loc[v4_rests, harm.columns[4]].astype(str)\n",
    "        v3_rests = v4_rests & (harm.iloc[:,3] == '0') & (harm.iloc[:,4] == '0')\n",
    "        harm.loc[v3_rests, 'figures'] = harm.loc[v3_rests, harm.columns[0]].astype(str) + harm.loc[v3_rests, harm.columns[3]].astype(str) + harm.loc[v3_rests, harm.columns[5]].astype(str)\n",
    "    \n",
    "    elif voice_number == 5:\n",
    "        v5_rests = (harm.iloc[:,0] == '0') & (harm.iloc[:,1] == '0') & (harm.iloc[:,2] == '0') & (harm.iloc[:,3] == '0') \n",
    "        harm.loc[v5_rests, 'figures'] = harm.loc[v5_rests, harm.columns[0]].astype(str) + harm.loc[v5_rests, harm.columns[4]].astype(str) + harm.loc[v5_rests, harm.columns[5]].astype(str) + harm.loc[v5_rests, harm.columns[6]].astype(str)\n",
    "        v4_rests =  v5_rests & (harm.iloc[:,4] == '0') & (harm.iloc[:,5] == '0') & (harm.iloc[:,6] == '0')\n",
    "        harm.loc[v4_rests, 'figures'] = harm.loc[v4_rests, harm.columns[0]].astype(str) + harm.loc[v4_rests, harm.columns[4]].astype(str) + harm.loc[v4_rests, harm.columns[7]].astype(str)  + harm.loc[v4_rests, harm.columns[8]].astype(str)\n",
    "        v3_rests =  v4_rests & (harm.iloc[:,7] == '0') & (harm.iloc[:,8] == '0')\n",
    "        harm.loc[v3_rests, 'figures'] = harm.loc[v3_rests, harm.columns[0]].astype(str) + harm.loc[v3_rests, harm.columns[4]].astype(str) + harm.loc[v3_rests, harm.columns[7]].astype(str)  + harm.loc[v3_rests, harm.columns[9]].astype(str)\n",
    "    \n",
    "    elif voice_number == 6:\n",
    "        v6_rests = (harm.iloc[:,0] == '0') & (harm.iloc[:,1] == '0') & (harm.iloc[:,2] == '0') & (harm.iloc[:,3] == '0') & (harm.iloc[:,4] == '0') \n",
    "        harm.loc[v6_rests, 'figures'] = harm.loc[v6_rests, harm.columns[0]].astype(str) + harm.loc[v6_rests, harm.columns[5]].astype(str) + harm.loc[v6_rests, harm.columns[6]].astype(str) + harm.loc[v6_rests, harm.columns[7]].astype(str) + harm.loc[v6_rests, harm.columns[8]].astype(str)\n",
    "        v5_rests = v6_rests & (harm.iloc[:,5] == '0') & (harm.iloc[:,6] == '0') & (harm.iloc[:,7] == '0') & (harm.iloc[:,8] == '0') \n",
    "        harm.loc[v5_rests, 'figures'] = harm.loc[v5_rests, harm.columns[0]].astype(str) + harm.loc[v5_rests, harm.columns[5]].astype(str) + harm.loc[v5_rests, harm.columns[9]].astype(str) + harm.loc[v5_rests, harm.columns[10]].astype(str) + harm.loc[v5_rests, harm.columns[11]].astype(str)\n",
    "        v4_rests =  v5_rests & (harm.iloc[:,9] == '0') & (harm.iloc[:,10] == '0') & (harm.iloc[:,11] == '0')\n",
    "        harm.loc[v4_rests, 'figures'] = harm.loc[v4_rests, harm.columns[0]].astype(str) + harm.loc[v4_rests, harm.columns[5]].astype(str) + harm.loc[v4_rests, harm.columns[9]].astype(str) + harm.loc[v4_rests, harm.columns[12]].astype(str) + harm.loc[v4_rests, harm.columns[13]].astype(str)\n",
    "        v3_rests =  v4_rests & (harm.iloc[:,12] == '0') & (harm.iloc[:,13] == '0')\n",
    "        harm.loc[v3_rests, 'figures'] = harm.loc[v3_rests, harm.columns[0]].astype(str) + harm.loc[v3_rests, harm.columns[5]].astype(str) + harm.loc[v3_rests, harm.columns[9]].astype(str) + harm.loc[v3_rests, harm.columns[12]].astype(str) + harm.loc[v3_rests, harm.columns[14]].astype(str)\n",
    "    \n",
    "    \n",
    "    harm = harm.loc[harm['figures'].shift() != harm['figures']] # getting rid of consecutive duplicate rows\n",
    "    for i in range(1, quote_length): #creating as many new 'figures' columns as quote length requires\n",
    "        harm[f'figures{i}'] = harm['figures'].shift(-i) #or \"figures\"+str(i)\n",
    "    harm['figures_combined'] = harm[harm.columns[-quote_length:]].iloc[0:].values.tolist() #combining all 'figures{i}' as a list\n",
    "   \n",
    "    harm = harm.dropna().drop(harm.columns[-quote_length:-1], axis=1) #drop a row if contains NaN values, then drop all 'figures{i}' columns\n",
    "    cleaned_list.append(harm)\n",
    "    \n",
    "harm_corpus = pd.concat(cleaned_list)\n",
    "harm_corpus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c64a0c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>figures_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>['538', '532', '533', '534', '835', '247', '358', '584', '583', '838']</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['838', '833', '823', '038', '047', '583', '472', '388', '287', '386']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['735', '584', '583', '838', '833', '823', '038', '047', '583', '472']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['583', '835', '724', '683', '572', '358', '583', '636', '853', '385']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['533', '534', '835', '247', '358', '584', '583', '838', '538', '530']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['584', '583', '838', '833', '823', '038', '047', '583', '472', '388']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['583', '838', '833', '823', '038', '047', '583', '472', '388', '287']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['534', '835', '247', '358', '584', '583', '838', '538', '530', '538']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['835', '735', '584', '583', '838', '833', '823', '038', '047', '583']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['247', '358', '584', '583', '838', '538', '530', '538', '053', '853']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['835', '247', '358', '584', '583', '838', '538', '530', '538', '053']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['833', '823', '038', '047', '583', '472', '388', '287', '386', '385']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['853', '538', '532', '533', '534', '835', '247', '358', '584', '583']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['532', '533', '534', '835', '247', '358', '584', '583', '838', '538']</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['838', '538', '530', '538', '053', '853', '844', '835', '825', '385']</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['472', '388', '287', '386', '385', '853', '852', '835', '735', '584']</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['852', '835', '735', '584', '583', '838', '833', '823', '038', '047']</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['583', '838', '538', '530', '538', '053', '853', '844', '835', '825']</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['538', '053', '853', '844', '835', '825', '385', '375', '853', '852']</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['358', '584', '583', '838', '538', '530', '538', '053', '853', '844']</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        figures_combined\n",
       "['538', '532', '533', '534', '835', '247', '358', '584', '583', '838']                 4\n",
       "['838', '833', '823', '038', '047', '583', '472', '388', '287', '386']                 3\n",
       "['735', '584', '583', '838', '833', '823', '038', '047', '583', '472']                 3\n",
       "['583', '835', '724', '683', '572', '358', '583', '636', '853', '385']                 3\n",
       "['533', '534', '835', '247', '358', '584', '583', '838', '538', '530']                 3\n",
       "['584', '583', '838', '833', '823', '038', '047', '583', '472', '388']                 3\n",
       "['583', '838', '833', '823', '038', '047', '583', '472', '388', '287']                 3\n",
       "['534', '835', '247', '358', '584', '583', '838', '538', '530', '538']                 3\n",
       "['835', '735', '584', '583', '838', '833', '823', '038', '047', '583']                 3\n",
       "['247', '358', '584', '583', '838', '538', '530', '538', '053', '853']                 3\n",
       "['835', '247', '358', '584', '583', '838', '538', '530', '538', '053']                 3\n",
       "['833', '823', '038', '047', '583', '472', '388', '287', '386', '385']                 3\n",
       "['853', '538', '532', '533', '534', '835', '247', '358', '584', '583']                 3\n",
       "['532', '533', '534', '835', '247', '358', '584', '583', '838', '538']                 3\n",
       "['838', '538', '530', '538', '053', '853', '844', '835', '825', '385']                 2\n",
       "['472', '388', '287', '386', '385', '853', '852', '835', '735', '584']                 2\n",
       "['852', '835', '735', '584', '583', '838', '833', '823', '038', '047']                 2\n",
       "['583', '838', '538', '530', '538', '053', '853', '844', '835', '825']                 2\n",
       "['538', '053', '853', '844', '835', '825', '385', '375', '853', '852']                 2\n",
       "['358', '584', '583', '838', '538', '530', '538', '053', '853', '844']                 2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#harm_corpus.figures.value_counts().head(30) #.unique()\n",
    "# count exact quotes\n",
    "temp = harm_corpus[['Composer','Title','figures_combined']].astype(str) #to use .value_counts(), I need to convert entire list into a sinle string. Idk why can't use value_counts() with a list? \n",
    "temp['figures_combined'].value_counts()[temp['figures_combined'].value_counts()>1].to_frame().head(20)\n",
    "#locate exact quotes or quote fragments (substrings)\n",
    "#temp[temp['figures_combined'].str.contains(\"683', '572', '358', '583', '636', '853'\")] #each in '', no []; eg.(\"'583', '358', '362', '383', '324', '533', '523', '368', '373', '363'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3f5c227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>match</th>\n",
       "      <th>source_location</th>\n",
       "      <th>match_location</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[583, 358, 583, 358, 583, 050, 058, 803, 804, 633]</td>\n",
       "      <td>[358, 583, 358, 583, 050, 058, 803, 804, 633, 638]</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_1_1.0</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_2_1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[583, 358, 583, 358, 583, 050, 058, 803, 804, 633]</td>\n",
       "      <td>[583, 358, 583, 050, 058, 803, 804, 633, 638, 527]</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_1_1.0</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_2_2.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[583, 358, 583, 358, 583, 050, 058, 803, 804, 633]</td>\n",
       "      <td>[358, 583, 050, 058, 803, 804, 633, 638, 527, 335]</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_1_1.0</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_2_3.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[583, 358, 583, 358, 583, 050, 058, 803, 804, 633]</td>\n",
       "      <td>[583, 050, 058, 803, 804, 633, 638, 527, 335, 453]</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_1_1.0</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_3_1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[583, 358, 583, 358, 583, 050, 058, 803, 804, 633]</td>\n",
       "      <td>[050, 058, 803, 804, 633, 638, 527, 335, 453, 363]</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_1_1.0</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_3_3.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569773</th>\n",
       "      <td>[732, 385, 284, 853, 852, 538, 537, 835, 684, 583]</td>\n",
       "      <td>[284, 853, 852, 538, 537, 835, 684, 583, 482, 583]</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_3.5</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569774</th>\n",
       "      <td>[732, 385, 284, 853, 852, 538, 537, 835, 684, 583]</td>\n",
       "      <td>[853, 852, 538, 537, 835, 684, 583, 482, 583, 158]</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_3.5</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_23_1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569775</th>\n",
       "      <td>[385, 284, 853, 852, 538, 537, 835, 684, 583, 482]</td>\n",
       "      <td>[284, 853, 852, 538, 537, 835, 684, 583, 482, 583]</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.0</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569776</th>\n",
       "      <td>[385, 284, 853, 852, 538, 537, 835, 684, 583, 482]</td>\n",
       "      <td>[853, 852, 538, 537, 835, 684, 583, 482, 583, 158]</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.0</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_23_1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569777</th>\n",
       "      <td>[284, 853, 852, 538, 537, 835, 684, 583, 482, 583]</td>\n",
       "      <td>[853, 852, 538, 537, 835, 684, 583, 482, 583, 158]</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.5</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Agnus Dei_23_1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569778 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    source  \\\n",
       "0       [583, 358, 583, 358, 583, 050, 058, 803, 804, 633]   \n",
       "1       [583, 358, 583, 358, 583, 050, 058, 803, 804, 633]   \n",
       "2       [583, 358, 583, 358, 583, 050, 058, 803, 804, 633]   \n",
       "3       [583, 358, 583, 358, 583, 050, 058, 803, 804, 633]   \n",
       "4       [583, 358, 583, 358, 583, 050, 058, 803, 804, 633]   \n",
       "...                                                    ...   \n",
       "569773  [732, 385, 284, 853, 852, 538, 537, 835, 684, 583]   \n",
       "569774  [732, 385, 284, 853, 852, 538, 537, 835, 684, 583]   \n",
       "569775  [385, 284, 853, 852, 538, 537, 835, 684, 583, 482]   \n",
       "569776  [385, 284, 853, 852, 538, 537, 835, 684, 583, 482]   \n",
       "569777  [284, 853, 852, 538, 537, 835, 684, 583, 482, 583]   \n",
       "\n",
       "                                                     match  \\\n",
       "0       [358, 583, 358, 583, 050, 058, 803, 804, 633, 638]   \n",
       "1       [583, 358, 583, 050, 058, 803, 804, 633, 638, 527]   \n",
       "2       [358, 583, 050, 058, 803, 804, 633, 638, 527, 335]   \n",
       "3       [583, 050, 058, 803, 804, 633, 638, 527, 335, 453]   \n",
       "4       [050, 058, 803, 804, 633, 638, 527, 335, 453, 363]   \n",
       "...                                                    ...   \n",
       "569773  [284, 853, 852, 538, 537, 835, 684, 583, 482, 583]   \n",
       "569774  [853, 852, 538, 537, 835, 684, 583, 482, 583, 158]   \n",
       "569775  [284, 853, 852, 538, 537, 835, 684, 583, 482, 583]   \n",
       "569776  [853, 852, 538, 537, 835, 684, 583, 482, 583, 158]   \n",
       "569777  [853, 852, 538, 537, 835, 684, 583, 482, 583, 158]   \n",
       "\n",
       "                                                source_location  \\\n",
       "0                           Pierre Sandrin_Doulce memoire_1_1.0   \n",
       "1                           Pierre Sandrin_Doulce memoire_1_1.0   \n",
       "2                           Pierre Sandrin_Doulce memoire_1_1.0   \n",
       "3                           Pierre Sandrin_Doulce memoire_1_1.0   \n",
       "4                           Pierre Sandrin_Doulce memoire_1_1.0   \n",
       "...                                                         ...   \n",
       "569773  Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_3.5   \n",
       "569774  Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_3.5   \n",
       "569775  Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.0   \n",
       "569776  Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.0   \n",
       "569777  Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.5   \n",
       "\n",
       "                                                 match_location  distance  \n",
       "0                           Pierre Sandrin_Doulce memoire_2_1.0         2  \n",
       "1                           Pierre Sandrin_Doulce memoire_2_2.0         4  \n",
       "2                           Pierre Sandrin_Doulce memoire_2_3.0         6  \n",
       "3                           Pierre Sandrin_Doulce memoire_3_1.0         8  \n",
       "4                           Pierre Sandrin_Doulce memoire_3_3.0        10  \n",
       "...                                                         ...       ...  \n",
       "569773  Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.5         4  \n",
       "569774  Roland de Lassus_Missa Doulce memoire: Agnus Dei_23_1.0         6  \n",
       "569775  Roland de Lassus_Missa Doulce memoire: Agnus Dei_22_4.5         2  \n",
       "569776  Roland de Lassus_Missa Doulce memoire: Agnus Dei_23_1.0         4  \n",
       "569777  Roland de Lassus_Missa Doulce memoire: Agnus Dei_23_1.0         2  \n",
       "\n",
       "[569778 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic similarity detector (Takes 2.5mins to load)\n",
    "harm_corpus = harm_corpus.reset_index()\n",
    "harm_corpus[\"Location\"] = harm_corpus[\"Composer\"] + \"_\" + harm_corpus[\"Title\"]+ \"_\" + harm_corpus[\"Measure\"].astype(str) + \"_\" + harm_corpus[\"Beat\"].astype(str)\n",
    "\n",
    "location_list = harm_corpus['Location'].tolist()\n",
    "location_list_comb = list(combinations(location_list, 2)) \n",
    "comb_location = pd.DataFrame(location_list_comb, columns = ['source_location', 'match_location']) \n",
    "\n",
    "figures_list = harm_corpus['figures_combined'].tolist()\n",
    "figures_list_comb = list(combinations(figures_list, 2)) \n",
    "comb_dist = pd.DataFrame(figures_list_comb, columns = ['source', 'match']) \n",
    "comb_dist['distance'] = comb_dist[['source', 'match']].apply(lambda packed_vars : td.levenshtein.distance(*packed_vars), axis=1)\n",
    "\n",
    "comb = pd.concat([comb_dist.reset_index(drop=True),comb_location.reset_index(drop=True)], axis=1)\n",
    "comb = comb[['source','match','source_location','match_location','distance']]\n",
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0a9aca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>match</th>\n",
       "      <th>source_location</th>\n",
       "      <th>match_location</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9567</th>\n",
       "      <td>[633, 638, 527, 335, 453, 363, 584, 583, 838, 583]</td>\n",
       "      <td>[638, 527, 335, 453, 363, 584, 583, 838, 583, 358]</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_4_3.0</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_4_4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>[633, 638, 527, 335, 453, 363, 584, 583, 838, 583]</td>\n",
       "      <td>[603, 638, 527, 335, 453, 363, 584, 583, 838, 135]</td>\n",
       "      <td>Pierre Sandrin_Doulce memoire_4_3.0</td>\n",
       "      <td>Roland de Lassus_Missa Doulce memoire: Kyrie_4_3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "9567  [633, 638, 527, 335, 453, 363, 584, 583, 838, 583]   \n",
       "9877  [633, 638, 527, 335, 453, 363, 584, 583, 838, 583]   \n",
       "\n",
       "                                                   match  \\\n",
       "9567  [638, 527, 335, 453, 363, 584, 583, 838, 583, 358]   \n",
       "9877  [603, 638, 527, 335, 453, 363, 584, 583, 838, 135]   \n",
       "\n",
       "                          source_location  \\\n",
       "9567  Pierre Sandrin_Doulce memoire_4_3.0   \n",
       "9877  Pierre Sandrin_Doulce memoire_4_3.0   \n",
       "\n",
       "                                          match_location  distance  \n",
       "9567                 Pierre Sandrin_Doulce memoire_4_4.0         2  \n",
       "9877  Roland de Lassus_Missa Doulce memoire: Kyrie_4_3.0         2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the above filtered, adjust variable\n",
    "comb[(comb['source_location'] == 'Pierre Sandrin_Doulce memoire_4_3.0') & (comb['distance'] < 3)]\n",
    "#df\n",
    "#df[df['distance'] < 4]\n",
    "\n",
    "#comb[comb['distance'] < 3]\n",
    "#comb.loc[(comb['distance'] > 0) & (comb['distance'] < 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31060d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af1babe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfabians_df = harm_corpus[['Composer','Title','figures_combined']].astype(str)\\nfabians_df = fabians_df['figures_combined'].value_counts()[fabians_df['figures_combined'].value_counts()>1].to_frame()\\n\\nfor index, row in fabians_df.iterrows():\\n    el1 = [ int(index[0][i]) for i in range(3) ]\\n    el2 = \\n\\n############\\n#create a dictionary for each figure to save computing power\\nmemos = {}\\ndef distance_func(source, match):\\n    key = tuple(source, match)\\n    if key in memos:\\n        return memos[key]\\n    elif tuple(match, source) in memos:\\n        return memos[tuple(match, source)]\\n    else:\\n        memos[key] = td.levenshtein.distance(*key)\\n        return memos[key]\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "\n",
    "'''\n",
    "fabians_df = harm_corpus[['Composer','Title','figures_combined']].astype(str)\n",
    "fabians_df = fabians_df['figures_combined'].value_counts()[fabians_df['figures_combined'].value_counts()>1].to_frame()\n",
    "\n",
    "for index, row in fabians_df.iterrows():\n",
    "    el1 = [ int(index[0][i]) for i in range(3) ]\n",
    "    el2 = \n",
    "\n",
    "############\n",
    "#create a dictionary for each figure to save computing power\n",
    "memos = {}\n",
    "def distance_func(source, match):\n",
    "    key = tuple(source, match)\n",
    "    if key in memos:\n",
    "        return memos[key]\n",
    "    elif tuple(match, source) in memos:\n",
    "        return memos[tuple(match, source)]\n",
    "    else:\n",
    "        memos[key] = td.levenshtein.distance(*key)\n",
    "        return memos[key]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8f4355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmy_quote = [my_quote for i in range(len(harm_corpus))]\\ndf = harm_corpus[[\\'Composer\\',\\'Title\\',\\'figures_combined\\']].copy() #.reset_index() #harm_corpus[\\'figures_combined\\'].to_frame() #harm_corpus[[\\'Composer\\',\\'Title\\',\\'figures_combined\\']].astype(str)\\ndf.rename(columns={\\'figures_combined\\':\\'similar_passage\\'}, inplace=True)\\ndf[\"my_quote\"] = my_quote\\ndf[\\'distance\\'] = df[[\\'similar_passage\\', \\'my_quote\\']].apply(lambda packed_vars : td.levenshtein.distance(*packed_vars), axis=1)\\ndf\\n#the above filtered, adjust variable\\ndf[df[\\'distance\\'] < 5]\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarity detector for given quote\n",
    "#my_quote =  ['363', '583', '830', '583', '835', '724', '683', '572', '358', '583', '636', '853'] # THIS IS A LIST #['583', '358', '583', '358', '583', '050', '058', '803'] \n",
    "\n",
    "'''\n",
    "my_quote = [my_quote for i in range(len(harm_corpus))]\n",
    "df = harm_corpus[['Composer','Title','figures_combined']].copy() #.reset_index() #harm_corpus['figures_combined'].to_frame() #harm_corpus[['Composer','Title','figures_combined']].astype(str)\n",
    "df.rename(columns={'figures_combined':'similar_passage'}, inplace=True)\n",
    "df[\"my_quote\"] = my_quote\n",
    "df['distance'] = df[['similar_passage', 'my_quote']].apply(lambda packed_vars : td.levenshtein.distance(*packed_vars), axis=1)\n",
    "df\n",
    "#the above filtered, adjust variable\n",
    "df[df['distance'] < 5]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25858eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRASH\n",
    "#type(harm_corpus['figures_combined']) #series\n",
    "#type(harm_corpus.iloc[0,9]) #list; each row in figures_combined is a list, even though each element within it isnt in ''\n",
    "#type(harm_corpus.iloc[0,9][0]) #str\n",
    "\n",
    "#my_list = ['A', 'B','C', 'D']\n",
    "#my_big_list = list(combinations(my_list, 2))\n",
    "#df_list = pd.DataFrame(my_big_list, columns = ['source', 'match'])\n",
    "#df_list\n",
    "\n",
    "#TESTING # This is working automatic similarity detector\n",
    "#df_corpus_comb_list = pd.DataFrame(corpus_comb_list, columns = ['source', 'match']) \n",
    "#df_corpus_comb_list\n",
    "    #type(df_corpus_comb_list.iloc[0,0][0])\n",
    "    #type(harm_corpus.iloc[0,9]) #list\n",
    "    #type(harm_corpus.iloc[0,9][0]) #str#TESTING\n",
    "#df_corpus_comb_list['similarity'] = df_corpus_comb_list[['source', 'match']].apply(lambda packed_vars : td.levenshtein.distance(*packed_vars), axis=1)\n",
    "#df_corpus_comb_list\n",
    "#filtered_dist = df_corpus_comb_list.loc[(df_corpus_comb_list['similarity'] > 0) & (df_corpus_comb_list['similarity'] < 4)]\n",
    "#filtered_dist\n",
    "\n",
    "\n",
    "# OLD similarity detector for given quote # how to not need [] and ''?? (each column in harm_corpus is object;harm_corpus.dtypes); #terribly slow, how to make faster? \n",
    "#my_quote = \"['724', '683', '572', '358', '583', '636', '853', '385']\" #['583', '835', '724', '683', '572', '358', '583', '636', '853', '385', '484', '584', '583', '838']\" #need [] and ''\n",
    "#df = harm_corpus[['Composer','Title','figures_combined']].astype(str)\n",
    "#df.rename(columns={'figures_combined':'similar_passage'}, inplace=True)\n",
    "#df[\"my_quote\"] = my_quote\n",
    "#df['similarity'] = df[['similar_passage', 'my_quote']].apply(lambda packed_vars : td.levenshtein.distance(*packed_vars), axis=1)\n",
    "\n",
    "#automatic similarity detector via generating lists with ngrams method, ngram_figures_corpus\n",
    "#dist = piece.distance(df=ngram_figures_corpus).stack().to_frame() #note that all one column, \"0\" with the similarity value\n",
    "#dist = dist.reset_index()\n",
    "#dist.rename(columns =  {'level_0':'source', 'level_1':'match'}, inplace = True) # doesnt work: , '0':'similarity'\n",
    "#dist\n",
    "\n",
    "#old version, which ties into same list end of one movement with beginning of next\n",
    "#df = harm_corpus['figures'].to_frame()\n",
    "#ngrams = piece.ngrams(df=df, n=10) #, exclude=['Rest']\n",
    "#dist = piece.distance(df=ngrams).stack().to_frame()\n",
    "#dist\n",
    "\n",
    "\n",
    "#filter automatic similarity detector:\n",
    "#set_distance_threshold = 3\n",
    "#distance_factor = set_distance_threshold + 1\n",
    "#dist.loc[x, \"Duration\"] > 120\n",
    "#filtered_dist = dist[dist[0] < distance_factor] # make it 0<dist[0]\n",
    "#filtered_dist\n",
    "        \n",
    "#    = filtered_dist.reset_index()\n",
    "#filtered_dist.rename(columns =  {'level_0':'source', 'level_1':'match'}, inplace = True) # , '0':'similarity' doesnt work\n",
    "#filtered_dist\n",
    "#currently gives distance 3 if one element changes, 838 =>835\n",
    "#also, changes order of source, but I want source to be ordered\n",
    "#have source keep its measure, beat numbers \n",
    "\n",
    "#For ngram method in creating harm_corpus    \n",
    "    #ngram_figures = harm['figures'].to_frame() #for ngram similarity method\n",
    "    #ngram_figures = piece.ngrams(df=ngram_figures, n=quote_length) #for ngram similarity method\n",
    "    #cleaned_list2.append(ngram_figures) #for ngram similarity method\n",
    "    #cleaned_list2 = []\n",
    "#ngram_figures_corpus = pd.concat(cleaned_list2) #for ngram similarity method\n",
    "\n",
    "#TRASH, I THINK: entire cartesian product, works but only for small number thus useless; must cut duplicates\n",
    "#df1 = harm_corpus[['Composer','Title','figures_combined']].astype(str)\n",
    "#df2 = harm_corpus['figures_combined'].astype(str).to_frame()\n",
    "#df3 = pd.merge(df1.assign(key=1), df2.assign(key=1), on='key').drop('key', axis=1) # GETS RID OF BAR, MEASURE NUMBERS      pd.concat([df1,df2], axis=1)\n",
    "#df3 #= df3.iloc[0:500]\n",
    "#df3\n",
    "#df3['similarity'] = df3[['figures_combined_x', 'figures_combined_y']].apply(lambda packed_vars : td.levenshtein.distance(*packed_vars), axis=1)\n",
    "#df3[df3['similarity'] < 10]\n",
    "\n",
    "\n",
    "#df2_1 = harm_corpus['figures_combined'].astype(str).to_frame()\n",
    "#df2_2 = df2_1.reset_index(level=[0,1, 2])\n",
    "#df2_3 = df2_2 .drop(df2_2.columns[0:3], axis = 1)\n",
    "\n",
    "#new_df = harm_corpus['figures_combined'].to_frame()\n",
    "#df1 = new_df.iloc[0:10]\n",
    "#df2 = df1\n",
    "#df3 = pd.merge(df1.assign(key=1), df2.assign(key=1), on='key').drop('key', axis=1)\n",
    "#df3['similarity'] = df3[['figures_combined_x', 'figures_combined_y']].apply(lambda packed_vars : td.levenshtein.distance(*packed_vars), axis=1)\n",
    "#df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebd55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquote_length = 10\\n\\nfunc1 = ImportedPiece.harmonic\\nlist_of_dfs = corpus.batch(func=func1, kwargs={\\'kind\\': \\'d\\', \\'compound\\':False}, metadata=False)\\nfunc2 = ImportedPiece.detailIndex\\nlist_of_detail_index = corpus.batch(func=func2, kwargs={\\'offset\\':True,\\'df\\': list_of_dfs})\\n\\ncleaned_list = []\\nfor harm in list_of_detail_index: #for each df sitting in the list, arbitrarily called harm, applying these functions (for each df in list, can apply df functions, but not on list as a whole)\\n    harm = harm.fillna(method=\"pad\")\\n    harm = harm.replace(\\'Rest\\', 0)\\n    harm[\\'figures\\'] = harm[harm.columns[:-2]].apply( # 0: takes all the columns\\n        lambda x: \\'\\'.join(x.astype(str)), \\n        axis=1\\n    )\\n    harm = harm.loc[harm[\\'figures\\'].shift() != harm[\\'figures\\']]\\n    for i in range(1, quote_length):\\n        harm[f\"figures{i}\"] = harm[\\'figures\\'].shift(-i) #or \"figures\"+str(i)\\n    harm[\"figures_combined\"] = harm[harm.columns[-quote_length:]].apply( \\n        lambda x: \\'_\\'.join(x.astype(str)),\\n        axis=1\\n    )\\n    harm = harm.drop(harm.columns[-quote_length:-1], axis=1)\\n    cleaned_list.append(harm)\\n\\nharm_corpus = pd.concat(cleaned_list)\\nharm_corpus\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#DONT TOUCH; LEWIS'S ORIGINAL, WORKS\n",
    "\n",
    "'''\n",
    "quote_length = 10\n",
    "\n",
    "func1 = ImportedPiece.harmonic\n",
    "list_of_dfs = corpus.batch(func=func1, kwargs={'kind': 'd', 'compound':False}, metadata=False)\n",
    "func2 = ImportedPiece.detailIndex\n",
    "list_of_detail_index = corpus.batch(func=func2, kwargs={'offset':True,'df': list_of_dfs})\n",
    "\n",
    "cleaned_list = []\n",
    "for harm in list_of_detail_index: #for each df sitting in the list, arbitrarily called harm, applying these functions (for each df in list, can apply df functions, but not on list as a whole)\n",
    "    harm = harm.fillna(method=\"pad\")\n",
    "    harm = harm.replace('Rest', 0)\n",
    "    harm['figures'] = harm[harm.columns[:-2]].apply( # 0: takes all the columns\n",
    "        lambda x: ''.join(x.astype(str)), \n",
    "        axis=1\n",
    "    )\n",
    "    harm = harm.loc[harm['figures'].shift() != harm['figures']]\n",
    "    for i in range(1, quote_length):\n",
    "        harm[f\"figures{i}\"] = harm['figures'].shift(-i) #or \"figures\"+str(i)\n",
    "    harm[\"figures_combined\"] = harm[harm.columns[-quote_length:]].apply( \n",
    "        lambda x: '_'.join(x.astype(str)),\n",
    "        axis=1\n",
    "    )\n",
    "    harm = harm.drop(harm.columns[-quote_length:-1], axis=1)\n",
    "    cleaned_list.append(harm)\n",
    "\n",
    "harm_corpus = pd.concat(cleaned_list)\n",
    "harm_corpus\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
